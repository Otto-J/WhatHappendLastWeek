# What Happened Last Week

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäºŽ [Elysia](https://elysiajs.com/) + Bun çš„æ’­å®¢å‘¨æŠ¥æœåŠ¡ï¼Œè‡ªåŠ¨æŠ“å–å¹¶æ±‡æ€»ä¸Šå‘¨å„å¤§å‰ç«¯/æŠ€æœ¯æ’­å®¢çš„æœ€æ–°èŠ‚ç›®ï¼Œæ”¯æŒ RSS è¾“å‡ºå’ŒéŸ³é¢‘æ‰¹é‡ä¸‹è½½ã€‚

## ðŸ“¡ RSS è®¢é˜…åœ°å€

**ç›´æŽ¥è®¢é˜…æœ¬å‘¨æ’­å®¢æ›´æ–°ï¼š**

```
https://github.com/Otto-J/WhatHappendLastWeek/blob/main/rss.xml
```

æˆ–è€…è®¿é—®åœ¨çº¿é¢„è§ˆï¼š
ðŸŒ **[ç‚¹å‡»æŸ¥çœ‹ç²¾ç¾Žçš„ RSS Feed é¡µé¢](https://github.com/Otto-J/WhatHappendLastWeek/blob/main/rss.xml)**

> ðŸ’¡ æç¤ºï¼šå°†ä¸Šè¿°é“¾æŽ¥æ·»åŠ åˆ°ä½ å–œæ¬¢çš„ RSS é˜…è¯»å™¨ï¼ˆå¦‚ Feedlyã€Inoreaderã€NetNewsWire ç­‰ï¼‰å³å¯è®¢é˜…æ¯å‘¨æ’­å®¢æ›´æ–°ï¼

---

æ³¨æ„ï¼šè€ƒè™‘åˆ°éœ€è¦è‰¯å¥½çš„ç½‘ç»œçŽ¯å¢ƒã€æ¼«é•¿çš„éŸ³é¢‘ä¸‹è½½ã€è¿è¡Œé¢‘æ¬¡æ¯”è¾ƒä½Žï¼Œæœ¬åº”ç”¨æ›´é€‚åˆæœ¬åœ°è¿è¡Œã€‚

## ä¸»è¦ç‰¹æ€§

- è‡ªåŠ¨æŠ“å–å¹¶æ±‡æ€»ä¸Šå‘¨å„å¤§æ’­å®¢çš„æœ€æ–°èŠ‚ç›®
- æ”¯æŒ RSS è¾“å‡ºï¼Œæ–¹ä¾¿è®¢é˜…
- æ”¯æŒä¸€é”®æ‰¹é‡ä¸‹è½½éŸ³é¢‘
- Docker æ”¯æŒï¼Œæ˜“äºŽéƒ¨ç½²

---

## ä¾èµ–

- [Bun](https://bun.sh/) 1.2.3+
- Node.jsï¼ˆéƒ¨åˆ†ä¾èµ–å¦‚ `fs`ã€`stream`ï¼‰
- ä¸»è¦ä¾èµ–è§ `package.json`ï¼Œå¦‚ `elysia`, `rss-parser`, `dayjs`, `jstoxml` ç­‰

---

## å¿«é€Ÿå¼€å§‹

### æœ¬åœ°å¼€å‘

1. å®‰è£…ä¾èµ–

   ```sh
   bun install
   ```

2. å¯åŠ¨å¼€å‘æœåŠ¡å™¨

   ```sh
   bun run dev
   ```

   é»˜è®¤ç›‘å¬ 3000 ç«¯å£ã€‚

### Docker éƒ¨ç½²

éž arm èŠ¯ç‰‡ç”µè„‘ï¼š

```sh
docker pull oven/bun:1.2.3-alpine --platform linux/amd64
```

arm èŠ¯ç‰‡ç”µè„‘ï¼š

```sh
docker pull oven/bun:1.2.3-alpine --platform linux/arm64/v8
```

æž„å»ºå¹¶è¿è¡Œï¼š

```sh
bun run docker:build
bun run docker:run
```

---

## ä¸»è¦ API

### 1. `POST /fetchLastWeekPodcast`

- åŠŸèƒ½ï¼šæŠ“å–ä¸Šå‘¨æ‰€æœ‰æ’­å®¢çš„æœ€æ–°èŠ‚ç›®ï¼Œç»“æžœå†™å…¥ `results/{week}.json`ï¼Œè¿”å›žæœ¬å‘¨æ•°æ®ã€‚
- è¿”å›žç¤ºä¾‹ï¼š

  ```json
  {
    "status": true,
    "file": "24.json",
    "msg": "å†™å…¥æˆåŠŸ",
    "startOfWeek": "2024-06-10",
    "weekNumber": 24,
    "results": [
      {
        "feedTitle": "xxx",
        "updateStatus": 2,
        "data": [
          {
            "itemTitle": "xxx",
            "media": "http://...",
            "itemLink": "http://..."
          }
        ]
      }
    ]
  }
  ```

### 2. `POST /fetchMp3`

- åŠŸèƒ½ï¼šæ‰¹é‡ä¸‹è½½æœ¬å‘¨æ‰€æœ‰æœ‰éŸ³é¢‘é“¾æŽ¥çš„èŠ‚ç›®ï¼ŒéŸ³é¢‘ä¿å­˜åˆ° `media/{week}/` ç›®å½•ã€‚
- è¯·æ±‚ä½“å¯é€‰å‚æ•°ï¼š`weekNumber`
- è¿”å›žï¼šä¸‹è½½æˆåŠŸ/å¤±è´¥çš„è¯¦ç»†åˆ—è¡¨

### 3. `GET /rss`

- åŠŸèƒ½ï¼šè¾“å‡ºæœ¬å‘¨æ‰€æœ‰æ’­å®¢èŠ‚ç›®çš„ RSSï¼Œæ”¯æŒæ ‡å‡†è®¢é˜…ã€‚
- è¿”å›žï¼š`application/xml` æ ¼å¼çš„ RSS å†…å®¹

### 4. `GET /`

- å¥åº·æ£€æŸ¥ï¼Œè¿”å›ž `"Hello Elysia"`

---

## ç›®å½•ç»“æž„

- `src/` ä¸»è¦æºç 
  - `index.ts` å…¥å£ï¼Œæ³¨å†Œè·¯ç”±
  - `service/` ä¸šåŠ¡é€»è¾‘
  - `utils/` å·¥å…·å‡½æ•°
- `results/` æ¯å‘¨æ’­å®¢æ•°æ®ç¼“å­˜
- `media/` ä¸‹è½½çš„éŸ³é¢‘æ–‡ä»¶
- `scripts/lastWeek.ts` æŠ“å–æ’­å®¢çš„æ ¸å¿ƒè„šæœ¬

---

## å¸¸è§é—®é¢˜

- **å¦‚ä½•æ·»åŠ æ–°çš„æ’­å®¢æºï¼Ÿ**
  ç¼–è¾‘ `scripts/lastWeek.ts` ä¸­çš„ `rssList` æ•°ç»„ï¼Œæ·»åŠ æ–°çš„ RSS é“¾æŽ¥å³å¯ã€‚

- **å¦‚ä½•æ›´æ¢ç«¯å£ï¼Ÿ**
  ä¿®æ”¹ `src/index.ts` ä¸­çš„ `.listen(3000)`ã€‚

---
## Podcast MP3 Download Workflow

This project includes a GitHub Action workflow (`.github/workflows/download-mp3s.yml`) that automates the downloading of podcast MP3 files and uploads them to an S3-compatible storage service.

The process is as follows:
1.  The workflow is triggered manually or automatically after the "Update Weekly Podcast JSON" workflow completes.
2.  MP3 files referenced in the latest weekly JSON data are downloaded to a local `./media/` directory within the GitHub Action runner by the `scripts/downloadMp3sToLocal.ts` script.
3.  The entire content of the `./media/` directory is then uploaded to the specified S3 bucket using the `npx @web.worker/s3-upload-folder` CLI tool.

### Triggers

- **Manual:** Can be triggered manually from the GitHub Actions tab (`workflow_dispatch`).
- **Automatic:** Runs automatically after the "Update Weekly Podcast JSON" workflow completes successfully on the `main` branch (`workflow_run`).

### S3 Configuration via GitHub Secrets

To use this workflow for uploading to S3, you need to configure the following secrets in your GitHub repository settings (Settings > Secrets and variables > Actions > New repository secret):

-   `S3_ENDPOINT_URL`: The full endpoint URL for your S3-compatible storage (e.g., `https://your-s3-provider.com`).
-   `S3_REGION`: The AWS region of your bucket (e.g., `us-east-1`). This might be a default value if your S3-compatible service doesn't strictly use AWS regions.
-   `S3_ACCESS_KEY_ID`: Your S3 access key ID.
-   `S3_SECRET_ACCESS_KEY`: Your S3 secret access key.
-   `S3_BUCKET_NAME`: The name of the S3 bucket where the MP3 files will be stored.
-   `S3_PATH_PREFIX` (Optional): A path prefix (folder) within the bucket to store the MP3 files (e.g., `podcasts/`). If not provided, it defaults to `/` (root of the bucket).

### Scripts Involved

- **`scripts/downloadMp3sToLocal.ts`**: This script (run via `bun run download-mp3s`) handles downloading MP3s from URLs found in the JSON data into the local `./media/` directory.
- **`@web.worker/s3-upload-folder`**: This CLI tool (executed via `npx`) is used by the workflow to upload the contents of the `./media/` directory. The specific command used in the workflow is:
  ```bash
  npx @web.worker/s3-upload-folder \
    --dist ./media \
    --ak "${{ secrets.S3_ACCESS_KEY_ID }}" \
    --sk "${{ secrets.S3_SECRET_ACCESS_KEY }}" \
    --endpoint "${{ secrets.S3_ENDPOINT_URL }}" \
    --region "${{ secrets.S3_REGION }}" \
    --bucket "${{ secrets.S3_BUCKET_NAME }}" \
    --prefix "${{ secrets.S3_PATH_PREFIX || '/' }}"
  ```

You can test the download part locally by running `bun run download-mp3s`. This will only download files to a local `./media/` directory and will not perform any S3 upload.
